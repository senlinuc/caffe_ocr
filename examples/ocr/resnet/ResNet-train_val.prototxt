# Enter your network definition here.
# Use Shift+Enter to update the visualization.
name: "ResNet-18-train"

layer {
  name: "data"
  #type: "ImageData"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value:152
    mean_value:152
    mean_value:152
  }
  #image_data_param {
  data_param {
    source: "../dbtrain"
	#root_folder: "../images/"
    batch_size: 32
	rand_skip: 1000000
	#shuffle: true
  }
}

layer {
  name: "data"
  #type: "ImageData"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_value:152
    mean_value:152
    mean_value:152
  }
  #image_data_param {
  data_param {
    source: "../dbtest"
	#root_folder: "../images/"
    batch_size: 32
	#shuffle: true
	rand_skip: 100000
  }
}

layer {
  bottom: "data"
    top: "conv1"
    name: "conv1"
    type: "Convolution"
    convolution_param {
    num_output: 64
      kernel_size: 5
      pad: 2
      stride: 2
	  weight_filler { type: "msra" std: 0.010 }
	  bias_filler { type: "constant" value: 0 }
      }
}

layer {
  bottom: "conv1"
    top: "conv1"
    name: "bn_conv1"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "conv1"
    top: "conv1"
    name: "scale_conv1"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "conv1"
    top: "conv1"
    name: "conv1_relu"
    type: "ReLU"
    }

layer {
  bottom: "conv1"
    top: "pool1"
    name: "pool1"
    type: "Pooling"
    pooling_param {
    kernel_size: 3
      stride: 2
      pool: MAX
      }
}
##########################
######first shortcut######
##########################
layer {
  bottom: "pool1"
    top: "res2a_branch1"
    name: "res2a_branch1"
    type: "Convolution"
    convolution_param {
    num_output: 64
      kernel_size: 1
      pad: 0
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res2a_branch1"
    top: "res2a_branch1"
    name: "bn2a_branch1"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res2a_branch1"
    top: "res2a_branch1"
    name: "scale2a_branch1"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "pool1"
    top: "res2a_branch2a"
    name: "res2a_branch2a"
    type: "Convolution"
    convolution_param {
    num_output: 32
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res2a_branch2a"
    top: "res2a_branch2a"
    name: "bn2a_branch2a"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res2a_branch2a"
    top: "res2a_branch2a"
    name: "scale2a_branch2a"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res2a_branch2a"
    top: "res2a_branch2a"
    name: "res2a_branch2a_relu"
    type: "ReLU"
    }

layer {
  bottom: "res2a_branch2a"
    top: "res2a_branch2b"
    name: "res2a_branch2b"
    type: "Convolution"
    convolution_param {
    num_output: 64
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res2a_branch2b"
    top: "res2a_branch2b"
    name: "bn2a_branch2b"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res2a_branch2b"
    top: "res2a_branch2b"
    name: "scale2a_branch2b"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
    bottom: "res2a_branch1"
    bottom: "res2a_branch2b"
    top: "res2a"
    name: "res2a"
    type: "Eltwise"
    }

layer {
  bottom: "res2a"
    top: "res2a"
    name: "res2a_relu"
    type: "ReLU"
    }

layer {
    bottom: "res2a"
    top: "res2b_branch2a"
    name: "res2b_branch2a"
    type: "Convolution"
    convolution_param {
    num_output: 32
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res2b_branch2a"
    top: "res2b_branch2a"
    name: "bn2b_branch2a"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res2b_branch2a"
    top: "res2b_branch2a"
    name: "scale2b_branch2a"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res2b_branch2a"
    top: "res2b_branch2a"
    name: "res2b_branch2a_relu"
    type: "ReLU"
    }

layer {
  bottom: "res2b_branch2a"
    top: "res2b_branch2b"
    name: "res2b_branch2b"
    type: "Convolution"
    convolution_param {
    num_output: 64
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res2b_branch2b"
    top: "res2b_branch2b"
    name: "bn2b_branch2b"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res2b_branch2b"
    top: "res2b_branch2b"
    name: "scale2b_branch2b"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res2a"
    bottom: "res2b_branch2b"
    top: "res2b"
    name: "res2b"
    type: "Eltwise"
    }

layer {
  bottom: "res2b"
    top: "res2b"
    name: "res2b_relu"
    type: "ReLU"
    }


##########################
######second shortcut#####
##########################

layer {
  bottom: "res2b"
    top: "res3a_branch1"
    name: "res3a_branch1"
    type: "Convolution"
    convolution_param {
    num_output: 64
      kernel_size: 1
      pad: 0
      stride: 2
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res3a_branch1"
    top: "res3a_branch1"
    name: "bn3a_branch1"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res3a_branch1"
    top: "res3a_branch1"
    name: "scale3a_branch1"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res2b"
    top: "res3a_branch2a"
    name: "res3a_branch2a"
    type: "Convolution"
    convolution_param {
    num_output: 32
      kernel_size: 3
      pad: 1
      stride: 2
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res3a_branch2a"
    top: "res3a_branch2a"
    name: "bn3a_branch2a"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res3a_branch2a"
    top: "res3a_branch2a"
    name: "scale3a_branch2a"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res3a_branch2a"
    top: "res3a_branch2a"
    name: "res3a_branch2a_relu"
    type: "ReLU"
    }

layer {
  bottom: "res3a_branch2a"
    top: "res3a_branch2b"
    name: "res3a_branch2b"
    type: "Convolution"
    convolution_param {
    num_output: 64
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res3a_branch2b"
    top: "res3a_branch2b"
    name: "bn3a_branch2b"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res3a_branch2b"
    top: "res3a_branch2b"
    name: "scale3a_branch2b"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res3a_branch1"
    bottom: "res3a_branch2b"
    top: "res3a"
    name: "res3a"
    type: "Eltwise"
    }

layer {
  bottom: "res3a"
    top: "res3a"
    name: "res3a_relu"
    type: "ReLU"
    }


layer {
  bottom: "res3a"
    top: "res3b_branch2a"
    name: "res3b_branch2a"
    type: "Convolution"
    convolution_param {
    num_output: 64
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res3b_branch2a"
    top: "res3b_branch2a"
    name: "bn3b_branch2a"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res3b_branch2a"
    top: "res3b_branch2a"
    name: "scale3b_branch2a"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res3b_branch2a"
    top: "res3b_branch2a"
    name: "res3b_branch2a_relu"
    type: "ReLU"
    }

layer {
  bottom: "res3b_branch2a"
    top: "res3b_branch2b"
    name: "res3b_branch2b"
    type: "Convolution"
    convolution_param {
    num_output: 64
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res3b_branch2b"
    top: "res3b_branch2b"
    name: "bn3b_branch2b"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res3b_branch2b"
    top: "res3b_branch2b"
    name: "scale3b_branch2b"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res3a"
    bottom: "res3b_branch2b"
    top: "res3b"
    name: "res3b"
    type: "Eltwise"
    }

layer {
  bottom: "res3b"
    top: "res3b"
    name: "res3b_relu"
    type: "ReLU"
    }
##########################
######third shortcut#####
##########################

layer {
  bottom: "res3b"
    top: "res4a_branch1"
    name: "res4a_branch1"
    type: "Convolution"
    convolution_param {
    num_output: 128
      kernel_size: 1
      pad: 0
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res4a_branch1"
    top: "res4a_branch1"
    name: "bn4a_branch1"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res4a_branch1"
    top: "res4a_branch1"
    name: "scale4a_branch1"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res3b"
    top: "res4a_branch2a"
    name: "res4a_branch2a"
    type: "Convolution"
    convolution_param {
    num_output: 64
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res4a_branch2a"
    top: "res4a_branch2a"
    name: "bn4a_branch2a"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res4a_branch2a"
    top: "res4a_branch2a"
    name: "scale4a_branch2a"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res4a_branch2a"
    top: "res4a_branch2a"
    name: "res4a_branch2a_relu"
    type: "ReLU"
    }

layer {
  bottom: "res4a_branch2a"
    top: "res4a_branch2b"
    name: "res4a_branch2b"
    type: "Convolution"
    convolution_param {
    num_output: 128
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res4a_branch2b"
    top: "res4a_branch2b"
    name: "bn4a_branch2b"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res4a_branch2b"
    top: "res4a_branch2b"
    name: "scale4a_branch2b"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res4a_branch1"
    bottom: "res4a_branch2b"
    top: "res4a"
    name: "res4a"
    type: "Eltwise"
    }

layer {
  bottom: "res4a"
    top: "res4a"
    name: "res4a_relu"
    type: "ReLU"
    }

layer {
  bottom: "res4a"
    top: "res4b_branch2a"
    name: "res4b_branch2a"
    type: "Convolution"
    convolution_param {
    num_output: 64
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res4b_branch2a"
    top: "res4b_branch2a"
    name: "bn4b_branch2a"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res4b_branch2a"
    top: "res4b_branch2a"
    name: "scale4b_branch2a"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res4b_branch2a"
    top: "res4b_branch2a"
    name: "res4b_branch2a_relu"
    type: "ReLU"
    }

layer {
  bottom: "res4b_branch2a"
    top: "res4b_branch2b"
    name: "res4b_branch2b"
    type: "Convolution"
    convolution_param {
    num_output: 128
      kernel_size: 3
      pad: 1
      stride: 1
      bias_term: false
	  weight_filler { type: "msra" std: 0.010 }
      }
}

layer {
  bottom: "res4b_branch2b"
    top: "res4b_branch2b"
    name: "bn4b_branch2b"
    type: "BatchNorm"
    batch_norm_param {
    use_global_stats: false
      }
	param { lr_mult: 0 }
    param { lr_mult: 0 }
    param { lr_mult: 0 }
}

layer {
  bottom: "res4b_branch2b"
    top: "res4b_branch2b"
    name: "scale4b_branch2b"
    type: "Scale"
    scale_param {
    bias_term: true
      }
	param { lr_mult: 0 }
}

layer {
  bottom: "res4a"
    bottom: "res4b_branch2b"
    top: "res4b"
    name: "res4b"
    type: "Eltwise"
    }

layer {
  bottom: "res4b"
    top: "res4b"
    name: "res4b_relu"
    type: "ReLU"
    }

layer {
  name: "pool5_ave"
  type: "Pooling"
  bottom: "res4b"
  top: "pool5_ave"
  pooling_param {
    pool: AVE
    kernel_w: 1
	kernel_h: 4
    stride_w: 1
	stride_h: 1
  }
}

layer {
  name: "pool5_ave_transpose"
  top: "pool5_ave_transpose"
  bottom: "pool5_ave"
  type: "Transpose"
  transpose_param {
    dim: 3
    dim: 2
    dim: 0
    dim: 1
  }
}

layer {
  name: "blstm_input"
  type: "Reshape"
  bottom: "pool5_ave_transpose"
  top: "blstm_input"
  reshape_param {
    shape { dim: -1 }
    axis: 1
    num_axes: 2
  }
}

#===================blstm layer 1============================
#======lstm1===================
layer {
  name: "lstm1"
  type: "Lstm"
  bottom: "blstm_input"
  top: "lstm1"
  lstm_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}

# =====lstm1_reverse===================
layer {
  name: "lstm1-reverse1"
  type: "Reverse"
  bottom: "blstm_input"
  top: "rlstm1_input"
  reverse_param {
    axis: 0
  }
}
layer {
  name: "rlstm1"
  type: "Lstm"
  bottom: "rlstm1_input"
  top: "rlstm1-output"
  lstm_param {
    num_output: 256
	weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
   }
}
layer {
  name: "lstm1-reverse2"
  type: "Reverse"
  bottom: "rlstm1-output"
  top: "rlstm1"
  reverse_param {
    axis: 0
  }
}


# merge lstm1 and rlstm1
layer {
  name: "blstm1"
  type: "Concat"
  bottom: "lstm1"
  bottom: "rlstm1"
  top: "blstm1"
  concat_param {
    axis: 2
  }
}


 

#===================blstm layer 2============================
#======lstm2===================
layer {
  name: "lstm2"
  type: "Lstm"
  bottom: "blstm1"
  top: "lstm2"
  lstm_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}

# =====lstm2_reverse===================
layer {
  name: "lstm2-reverse1"
  type: "Reverse"
  bottom: "blstm1"
  top: "rlstm2_input"
  reverse_param {
    axis: 0
  }
}

layer {
  name: "rlstm2"
  type: "Lstm"
  bottom: "rlstm2_input"
  top: "rlstm2-output"
  lstm_param {
    num_output: 256
	weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
   }
}
layer {
  name: "lstm2-reverse2"
  type: "Reverse"
  bottom: "rlstm2-output"
  top: "rlstm2"
  reverse_param {
    axis: 0
  }
}

# merge lstm2 and rlstm2
layer {
  name: "blstm2"
  type: "Concat"
  bottom: "lstm2"
  bottom: "rlstm2"
  top: "blstm2"
  concat_param {
    axis: 2
  }
}

layer {
	name: "blstm_sum"
    type: "Eltwise"
    bottom: "blstm1"
    bottom: "blstm2"
    top: "blstm_sum"
    eltwise_param{
    	operation: SUM
    }
}

 


layer {
  name: "fc1x"
  type: "InnerProduct"
  bottom: "blstm_sum"
  top: "fc1x"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    axis: 2
    num_output: 5990
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}


layer {
  name: "ctcloss"
  type: "WarpCTCLoss"
  bottom: "fc1x"
  bottom: "label"
  top: "ctcloss"
  loss_weight:1
}

layer {
	name: "acc"
	type: "CTCGreedyDecoder"
	bottom: "fc1x"
	bottom: "label"
	top: "acc"
	include {
    phase: TEST
  }
}
